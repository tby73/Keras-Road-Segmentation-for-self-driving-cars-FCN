{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras import layers, losses, utils, optimizers, callbacks\n",
    "from keras.applications import VGG16\n",
    "from tqdm import tqdm\n",
    "from base64 import b64encode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get KITTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA_DIR = \"/input/kittiroadsegmentation/training/image_2/\"\n",
    "\n",
    "# ground trouth segmentation\n",
    "TRAIN_GT_DIR = \"/input/kittiroadsegmentation/training/gt_image_2/\"\n",
    "\n",
    "# test data\n",
    "TEST_DATA_DIR = \"/input/kittiroadsegmentation/testing/\"\n",
    "\n",
    "# get size of training samples\n",
    "TRAINSET_SIZE = int(len(os.listdir(TRAIN_DATA_DIR)) * 0.8)\n",
    "VALIDATION_SIZE = int(len(os.listdir(TRAIN_DATA_DIR)) * 0.1)\n",
    "TESTSET_SIZE = int(len(os.listdir(TRAIN_DATA_DIR)) - TRAINSET_SIZE - VALIDATION_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 1\n",
    "SEED = 123"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse data into masks and images as a dict, generation of dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseImages(image_path: str) -> dict:\n",
    "    # read standard road images and decoding\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # three types of image paths: (um_road, umm_road, uu_road)\n",
    "    mask_path = tf.strings.regex_replace(image_path, \"image_2\", \"gt_image_2\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"um_\", \"um_road_\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"umm_\", \"umm_road_\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"uu_\", \"uu_road_\")\n",
    "\n",
    "    # read and decode masks\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "\n",
    "    # labeling\n",
    "    non_road_label = np.array([255, 0, 0])\n",
    "    road_label = np.array([255, 0, 255])\n",
    "    other_road_label = np.array([0, 0, 0])\n",
    "\n",
    "    # convert mask to binary\n",
    "    mask = tf.experimental.numpy.all(mask == road_label, axis=2)\n",
    "    mask = tf.cast(mask, tf.uint8)\n",
    "    mask = tf.expand_dims(mask, axis=-1)\n",
    "\n",
    "    return { \"image\": image, \"segmentation_mask\": mask}\n",
    "\n",
    "# generate dataset splits: test, train, val\n",
    "dataset = tf.data.Dataset.list_files(TRAIN_DATA_DIR + \"*.png\", seed=SEED)\n",
    "dataset = dataset.map(ParseImages)\n",
    "\n",
    "# splitting\n",
    "train_ds = dataset.take(TRAINSET_SIZE + VALIDATION_SIZE)\n",
    "validation_ds = train_ds.skip(TRAINSET_SIZE)\n",
    "train_ds = train_ds.take(TRAINSET_SIZE)\n",
    "test_ds = dataset.skip(TRAINSET_SIZE - VALIDATION_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def Normalize(input_image, input_mask) -> tuple:\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image, input_mask\n",
    "\n",
    "@tf.function\n",
    "def LoadTrainImages(datapoint) -> tuple:\n",
    "    # resize images and masks\n",
    "    input_image = tf.image.resize(datapoint[\"image\"], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint[\"segmentation_mask\"], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # data augmentation by randomly flipping the image and generate new training data\n",
    "    if tf.random.uniform() > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    # normalize images\n",
    "    input_image, input_mask = Normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "@tf.function\n",
    "def load_image_test(datapoint: dict) -> tuple:\n",
    "    # resizing\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # normalizing\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
